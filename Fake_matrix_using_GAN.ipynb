{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFpllsQDSAwv1Q9R3vGo+l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/udayameister/Connectome/blob/main/Fake_matrix_using_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n"
      ],
      "metadata": {
        "id": "ShpLz_BltLir"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load real brain connectivity matrices from CSV\n",
        "real_data = pd.read_csv('real_data.csv', header=None)\n",
        "real_data = real_data.values  # Convert DataFrame to numpy array\n",
        "real_data = real_data.reshape(-1, 90, 90)  # Reshape to 90x90 matrices\n"
      ],
      "metadata": {
        "id": "IyqhQad2vpsv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define generator model\n",
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(256, input_shape=(100,)))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dense(90*90, activation='tanh'))\n",
        "    model.add(layers.Reshape((90, 90)))\n",
        "    return model"
      ],
      "metadata": {
        "id": "MKLuMJ-SvxKi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define discriminator model\n",
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Flatten(input_shape=(90, 90)))\n",
        "    model.add(layers.Dense(256))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dense(1))\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "mMbWeZ-Av9YC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define GAN model\n",
        "def make_gan_model(generator, discriminator):\n",
        "    discriminator.trainable = False\n",
        "    model = tf.keras.Sequential([generator, discriminator])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "zImbsY5vwDjJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create generator and discriminator models\n",
        "generator = make_generator_model()\n",
        "discriminator = make_discriminator_model()\n"
      ],
      "metadata": {
        "id": "6XnFkjEYwInC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create GAN model\n",
        "gan = make_gan_model(generator, discriminator)\n"
      ],
      "metadata": {
        "id": "QucA3eyQwLqC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "# Compile the models\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "metadata": {
        "id": "oi3dkCoFwPfL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(real_data, batch_size):\n",
        "    # Generate random noise as input to the generator\n",
        "    noise = tf.random.normal([batch_size, 100])\n",
        "    \n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        # Generate fake connectivity matrices using the generator\n",
        "        fake_data = generator(noise, training=True)\n",
        "        \n",
        "        # Concatenate real and fake data\n",
        "        combined_data = tf.concat([real_data, fake_data], axis=0)\n",
        "        \n",
        "        # Create labels for real and fake data\n",
        "        labels = tf.concat([tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0)\n",
        "        \n",
        "        # Train the discriminator\n",
        "        predictions = discriminator(combined_data, training=True)\n",
        "        disc_loss = loss_fn(labels, predictions)\n",
        "        \n",
        "        # Train the generator\n",
        "        gen_loss = loss_fn(tf.ones((batch_size, 1)), predictions[:batch_size])\n",
        "        \n",
        "    # Calculate gradients\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "    \n",
        "    # Apply gradients\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "    "
      ],
      "metadata": {
        "id": "8j5ZQXtYwRT5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "def train(dataset, epochs, batch_size):\n",
        "    for epoch in range(epochs):\n",
        "        for batch in dataset:\n",
        "            train_step(batch, batch_size)\n"
      ],
      "metadata": {
        "id": "LeQ1qTQHwWyp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the real data\n",
        "real_data = (real_data - np.min(real_data)) / (np.max(real_data) - np.min(real_data))\n"
      ],
      "metadata": {
        "id": "wCZP29U6wbIZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape real data for training\n",
        "real_data = real_data.reshape(real_data.shape[0], -1)"
      ],
      "metadata": {
        "id": "skZx0jwAweIh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "epochs = 1000\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "ki62GzEayu9A"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(real_data).batch(batch_size)"
      ],
      "metadata": {
        "id": "zikjilX7ywSY"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_fake_matrices = 10\n",
        "noise = tf.random.normal([num_fake_matrices, 100])\n",
        "fake_matrices = generator(noise, training=False)"
      ],
      "metadata": {
        "id": "HutacLgXy5ZS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fake_matrices = (fake_matrices * (np.max(real_data) - np.min(real_data))) + np.min(real_data)\n",
        "fake_matrices = fake_matrices.numpy().reshape(num_fake_matrices, 90, 90)"
      ],
      "metadata": {
        "id": "DW89OF8jy9-D"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, matrix in enumerate(fake_matrices):\n",
        "  pd.DataFrame(matrix).to_csv(f'fake_matrix_{i+1}.csv', index=False, header=False)"
      ],
      "metadata": {
        "id": "cIDpJtcszCeQ"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}